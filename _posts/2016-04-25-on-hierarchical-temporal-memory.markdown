---
layout: post
title:  "On Hierarchical Temporal Memory"
date:   2016-04-25
description: "On Jeff Hawkins Hierarchical Temporal Memory. What it is, how it works in principle, and why it is important."
---
Jeff Hawkins is a neuroscientist and one of the inventors of Hierarchical Temporal Memory (HTM) which is based on his idea of how the brain works. The theory is called memory-prediction framework and was introduced in 2004 in his book _On Intelligence_ and Numentas [white paper](http://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf). His main goal is to understand how the brain works in hopes to apply this knowledge to build intelligent machines which can solve problems which we have not been able to solve ourselves yet. **The technology he is researching is based on the neurological understanding of the human brain.** It focuses on the neocortex which is a sheet of cells which is wrapped around the rest of the brain and is responsible for the brains characteristic folds and lobes. This particular area of the brain can be flattened out into a sheet of roughly 50cm<sup>2</sup> with a thickness about 2.5mm containing approximately 30 billion neurones and is responsible for high-level vision, hearing, touch, movement, language, and planning. An important aspect of this sheet of neurones is the similar structure it is made of while performing those different high-level tasks suggesting that it follows a fundamental algorithm. Jeff thinks of the neocortex as a pattern recognizer processing the electrical signals from our senses into motor controls. The essential underlying structure he came up with is a hierarchical memory system to figure out causes in the world. The HTM nodes in the first layer work on sequences of data and try to find spatial patterns in them. These get passed on to the next level of the hierarchy where a new set of HTM nodes learn spatial patterns of sequences of the underlying patterns from before. An HTM creates a hierarchical model of causes which resolve ambiguity and can generalise due to sharing in its hierarchy. The following images are from Numentas white paper.

![A simplified view of the HTM hierarchy with several layers.](/images/hierarchy.png){: .center-image }

HTMs share similarities with other Machine Learning technologies such as Deep Belief Nets - an unsupervised artificial neural network. HTMs have so far been applied several real-world challenges such as credit card fraud detection and audio speaker identification. Their work is fully open source and there exist several companies which are successfully applying HTM technology in their business. Because of the time-dependent input data and the research focus on replicating biological principles we have not yet seen HTMs perform on common Machine Learning challenges. 

## HTM Under the Hood ##
HTMs model biologically plausible neurones which are called cells. Such cells are arranged in columns which are part of regions which belong to a certain level in a hierarchy. This topology is different from other Machine Learning algorithms using a neural network and is crucial to discover and infer high-level patterns.  HTMs are trained on lots of time varying data and rely on storing a large set of patterns and sequences. This is done by following the core principles of HTM which consist of the hierarchical organisation, the structure of HTM regions, the sparse distributed representation of data, and the use of time-based information.

The hierarchy of an HTM consists of several layers. Every layer is made up of several HTM regions. An HTM region is a unit of memory and prediction. The lower levels in the HTM cover low-level patterns which then are combined when the signal ascends into higher level layers. Multiple regions of a lower level always converge into a single region of the higher level. This organisation is efficient and significantly reduces training time and memory usage because patterns at each level can be reused and combined in different ways for patterns in higher levels. Sharing representations also leads to generalisation, especially at higher levels where very high-level representations enable new objects to inherit abstract properties of its sub-components. 

An HTM region is arranged in a two-dimensional collection of columns. Each column connects to a subset of the input and each cell connects to other cells in the region. That column structure inside a cell belongs to a specific region which is part of a specific layer. The cells inside an HTM region are highly interconnected but result in a sparse distributed representation. The sparse representation will change over time but will always constrain itself to a small percentage of active cells. It learns patterns in a purely statistical manner. First, it looks for activations which occur together. Those are named spatial patterns. It then searches sequences of these spatial patterns over time. Those are called temporal patterns. An HTM uses these learned patterns to perform inference on inputs it has never seen before, successfully matching them with the spatial and temporal patterns it knows. Thanks to the sparse distributed representation, the HTM is said to be excellent in handling noisy data just like our human brain is. The next image shows an HTM region with a two-dimensional structure of columns which is made up of individual HTM cells. The feed-forward data (black) and the expected predictions (gray) are important states of HTM cells.

![An HTM region with a two-dimensional structure of columns which is made up of individual HTM cells.](/images/region.png){: .center-image }

Prediction of future inputs is a key aspect of every HTM region on every level of the hierarchy. They are context sensitive because they include preceding inputs in their pattern matching process. The algorithms which work inside of an HTM region are called Cortical Learning Algorithms (CLA) or simply HTM learning algorithm. As explained before, HTM regions consist of a particular topology of HTM cells. These cells have a much more realistic neurone model than those found in other neural networks in the field of Machine Learning. A biological neurone consists of dendrites which are like branches containing many synapses in order to receive signals. The HTM cell is made up of two types of input dendrites. The single proximal dendrite segment receives a feed-forward input from regions in a lower layer according to the current timestep while a multitude of distal dendrites receive their input from neighbouring cells which belong to the same HTM region. All dendrites of a cell are made up of a number of synapses. Synapses have a binary on/off state with two additional attributes. The first is called potential synapses and refers to all the axons of other cells which could lead to new connections in the future. The second one is called permanence and is a floating point number from 0.0 to 1.0 and describes the strength of the connection. If the permanence of a neurone passes a threshold, it can flip and become a new connection. This increase and decrease in permanence is an important aspect of the Hebbian learning ability of HTMs. The following image shows an HTM neurone which  is much more biological plausible and is made up of one proximal dendrite (green) and several distal dendrites (blue).

![An HTM neurone which is made up of one proximal dendrite (green) and several distal dendrites (blue).](/images/neuron.png){: .center-image }

Cells of an HTM region have three different states. The cell can become active if it receives a strong activation from its distal dendrite (i.e. inputs from regions of the underlying level). The cell can get into a predictive state if it receives a strong activation from its proximal dendrites (i.e. inputs from neighbouring cells). The predictive state expects any cell in that column to be active during the next time step. If there is a distal dendrite activation but no according prediction from a proximal dendrite activation, then all the cells in that column will be activated. This "surprise" will lead to an increased permanence in the synapses of those cells which contributed to this effect. The exact reasons for a synapse update differ between distal and proximal dendrites but achieve a similar effect. 

## Conclusion ##
One of the main strengths of the HTM is its biological plausibility and the researchers, like Jeff Hawkins, who focus on building a computationally efficient model of how the neocortex works. Another important aspect is the focus on sequence data and its importance for the vision of building intelligent machines. The online training and unsupervised nature of HTM are further important and in my opinion positive aspects. 
Several companies have applied HTMs to make predictions on time-based input data, but there has been no public attempts or accomplishments in applying HTMs to popular and more practical Machine Learning challenges. Due to the lack of good and reproducible results, there has been little interest and excitement about the progress of HTMs in the mainstream artificial intelligence community. It also doesn't help that only very little work exists considering the mathematics behind HTM.
